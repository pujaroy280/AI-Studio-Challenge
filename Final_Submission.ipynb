{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bad Team's Final Submission for AMLC 2020 <a name='top'><a/>\n",
    "## Last update: Nov 6, 2020"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Outline\n",
    "1. [Module Import and Data Preprocessing](#m1)\n",
    "2. [LSTM Seq2Seq Modeling](#m2)\n",
    "3. [Predict for Phase 1](#m3)\n",
    "4. [Predict for Phase 2](#m4)\n",
    "5. [Evaluation by Committee](#m5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Module Import and Data Preprocessing <a name='m1'></a> \n",
    "[back to Top](#top)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "#from tqdm import tqdm\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "import boto3\n",
    "import io\n",
    "import os\n",
    "\n",
    "#from axp.ml.utils.fs.file_system import FileSystemFactory\n",
    "#from mls_mdk.utils.file_protocol_utils import FileProtocolUtility\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.utils.data\n",
    "\n",
    "\n",
    "#import seaborn as sns\n",
    "#from matplotlib import pyplot as plt\n",
    "#%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def seed_everything(seed=1903):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    \n",
    "seed_everything(seed=1903)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ref: https://github.com/Mrpatekful/swats/blob/master/swats/optim.py\n",
    "class SWATS(torch.optim.Optimizer):\n",
    "    r\"\"\"Implements Switching from Adam to SGD technique. Proposed in\n",
    "    `Improving Generalization Performance by Switching from Adam to SGD`\n",
    "    by Nitish Shirish Keskar, Richard Socher (2017).\n",
    "    The method applies Adam in the first phase of the training, then\n",
    "    switches to SGD when a criteria is met.\n",
    "    Implementation of Adam and SGD update are from `torch.optim.Adam` and\n",
    "    `torch.optim.SGD`.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, params, lr=1e-3, betas=(0.9, 0.999), eps=1e-8,\n",
    "                 weight_decay=0, amsgrad=False, verbose=False, \n",
    "                 nesterov=False):\n",
    "        if not 0.0 <= lr:\n",
    "            raise ValueError(\n",
    "                \"Invalid learning rate: {}\".format(lr))\n",
    "        if not 0.0 <= eps:\n",
    "            raise ValueError(\n",
    "                \"Invalid epsilon value: {}\".format(eps))\n",
    "        if not 0.0 <= betas[0] < 1.0:\n",
    "            raise ValueError(\n",
    "                \"Invalid beta parameter at index 0: {}\".format(betas[0]))\n",
    "        if not 0.0 <= betas[1] < 1.0:\n",
    "            raise ValueError(\n",
    "                \"Invalid beta parameter at index 1: {}\".format(betas[1]))\n",
    "        defaults = dict(lr=lr, betas=betas, eps=eps, phase='ADAM',\n",
    "                        weight_decay=weight_decay, amsgrad=amsgrad,\n",
    "                        verbose=verbose, nesterov=nesterov)\n",
    "\n",
    "        super().__init__(params, defaults)\n",
    "\n",
    "    def __setstate__(self, state):\n",
    "        super().__setstate__(state)\n",
    "        for group in self.param_groups:\n",
    "            group.setdefault('amsgrad', False)\n",
    "            group.setdefault('nesterov', False)\n",
    "            group.setdefault('verbose', False)\n",
    "\n",
    "    def step(self, closure=None):\n",
    "        \"\"\"Performs a single optimization step.\n",
    "        Arguments:\n",
    "            closure (callable, optional): \n",
    "                A closure that reevaluates the model\n",
    "                and returns the loss.\n",
    "        \"\"\"\n",
    "        loss = None\n",
    "        if closure is not None:\n",
    "            loss = closure()\n",
    "\n",
    "        for group in self.param_groups:\n",
    "            for w in group['params']:\n",
    "                if w.grad is None:\n",
    "                    continue\n",
    "                grad = w.grad.data\n",
    "\n",
    "                if grad.is_sparse:\n",
    "                    raise RuntimeError(\n",
    "                        'Adam does not support sparse gradients, '\n",
    "                        'please consider SparseAdam instead')\n",
    "\n",
    "                amsgrad = group['amsgrad']\n",
    "\n",
    "                state = self.state[w]\n",
    "\n",
    "                # state initialization\n",
    "                if len(state) == 0:\n",
    "                    state['step'] = 0\n",
    "                    # exponential moving average of gradient values\n",
    "                    state['exp_avg'] = torch.zeros_like(w.data)\n",
    "                    # exponential moving average of squared gradient values\n",
    "                    state['exp_avg_sq'] = torch.zeros_like(w.data)\n",
    "                    # moving average for the non-orthogonal projection scaling\n",
    "                    state['exp_avg2'] = w.new(1).fill_(0)\n",
    "                    if amsgrad:\n",
    "                        # maintains max of all exp. moving avg.\n",
    "                        # of sq. grad. values\n",
    "                        state['max_exp_avg_sq'] = torch.zeros_like(w.data)\n",
    "\n",
    "                exp_avg, exp_avg2, exp_avg_sq = \\\n",
    "                    state['exp_avg'], state['exp_avg2'], state['exp_avg_sq'],\n",
    "\n",
    "                if amsgrad:\n",
    "                    max_exp_avg_sq = state['max_exp_avg_sq']\n",
    "                beta1, beta2 = group['betas']\n",
    "\n",
    "                state['step'] += 1\n",
    "\n",
    "                if group['weight_decay'] != 0:\n",
    "                    grad.add_(group['weight_decay'], w.data)\n",
    "\n",
    "                # if its SGD phase, take an SGD update and continue\n",
    "                if group['phase'] == 'SGD':\n",
    "                    if 'momentum_buffer' not in state:\n",
    "                        buf = state['momentum_buffer'] = torch.clone(\n",
    "                            grad).detach()\n",
    "                    else:\n",
    "                        buf = state['momentum_buffer']\n",
    "                        buf.mul_(beta1).add_(grad)\n",
    "                        grad = buf\n",
    "\n",
    "                    grad.mul_(1 - beta1)\n",
    "                    if group['nesterov']:\n",
    "                        grad.add_(beta1, buf)\n",
    "\n",
    "                    w.data.add_(-group['lr'], grad)\n",
    "                    continue\n",
    "\n",
    "                # decay the first and second moment running average coefficient\n",
    "                exp_avg.mul_(beta1).add_(1 - beta1, grad)\n",
    "                exp_avg_sq.mul_(beta2).addcmul_(1 - beta2, grad, grad)\n",
    "                if amsgrad:\n",
    "                    # maintains the maximum of all 2nd\n",
    "                    # moment running avg. till now\n",
    "                    torch.max(max_exp_avg_sq, exp_avg_sq, out=max_exp_avg_sq)\n",
    "                    # use the max. for normalizing running avg. of gradient\n",
    "                    denom = max_exp_avg_sq.sqrt().add_(group['eps'])\n",
    "                else:\n",
    "                    denom = exp_avg_sq.sqrt().add_(group['eps'])\n",
    "\n",
    "                bias_correction1 = 1 - beta1 ** state['step']\n",
    "                bias_correction2 = 1 - beta2 ** state['step']\n",
    "                step_size = group['lr'] * \\\n",
    "                    (bias_correction2 ** 0.5) / bias_correction1\n",
    "\n",
    "                p = -step_size * (exp_avg / denom)\n",
    "                w.data.add_(p)\n",
    "\n",
    "                p_view = p.view(-1)\n",
    "                pg = p_view.dot(grad.view(-1))\n",
    "\n",
    "                if pg != 0:\n",
    "                    # the non-orthognal scaling estimate\n",
    "                    scaling = p_view.dot(p_view) / -pg\n",
    "                    exp_avg2.mul_(beta2).add_(1 - beta2, scaling)\n",
    "\n",
    "                    # bias corrected exponential average\n",
    "                    corrected_exp_avg = exp_avg2 / bias_correction2\n",
    "\n",
    "                    # checking criteria of switching to SGD training\n",
    "                    if state['step'] > 1 and \\\n",
    "                            corrected_exp_avg.allclose(scaling, rtol=1e-6) and \\\n",
    "                            corrected_exp_avg > 0:\n",
    "                        group['phase'] = 'SGD'\n",
    "                        group['lr'] = corrected_exp_avg.item()\n",
    "                        if group['verbose']:\n",
    "                            print('Switching to SGD after '\n",
    "                                  '{} steps with lr {:.5f} '\n",
    "                                  'and momentum {:.5f}.'.format(\n",
    "                                      state['step'], group['lr'], beta1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "def dump_pkl(model, scaler, sequence_in, filename):\n",
    "    \"\"\"prediction: df or numpy array; valid_loss: float; filename: str\"\"\"\n",
    "    output = dict()\n",
    "    output['model'] = model\n",
    "    output['scaler'] = scaler\n",
    "    output['sequence_in'] = sequence_in\n",
    "    with open(filename,  'wb') as f:\n",
    "        pickle.dump(output, f)\n",
    "\n",
    "def load_pkl(filename):\n",
    "    with open(filename, 'rb') as f:\n",
    "        return pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = %pwd\n",
    "data_path = Path(data_path+\"/data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv(Path(data_path, 'df_train.csv'), index_col='INTERVAL')\n",
    "df_train.sort_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_phase2 = pd.read_csv(Path(data_path, 'df_phase2.csv'), index_col='INTERVAL')\n",
    "df_phase2.sort_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_features=len(df_train.columns)\n",
    "n_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0.1 Data Smoothing\n",
    "[back to Top](#top)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_smooth(df, method='rolling_average', window=20):\n",
    "    \"\"\"Smooth the training data\"\"\"\n",
    "    if method == 'rolling_average':\n",
    "        return df.rolling(window).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0.2 Data Preparation\n",
    "[back to Top](#top)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# multivariate multi-step data preprocess (i.e. convert from time series problem to supervised learning problem)\n",
    "\n",
    "# set the number of future hours to predict\n",
    "hours_to_predict = 8\n",
    "\n",
    "# choose the number of time steps to be read in and predicted\n",
    "n_steps_in, n_steps_out = 96, int(4*hours_to_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. validation strategy: rolling window with sliding distance=1 DataPoint\n",
    "# split a multivariate sequence into samples\n",
    "def split_sequences(sequences, n_steps_in, n_steps_out):\n",
    "    X, y = list(), list()\n",
    "    for i in range(len(sequences)):\n",
    "        # find the end of this pattern\n",
    "        end_ix = i + n_steps_in\n",
    "        out_end_ix = end_ix + n_steps_out\n",
    "        # check if we are beyond the dataset\n",
    "        if out_end_ix > len(sequences):\n",
    "            break\n",
    "        # gather input and output parts of the pattern\n",
    "        seq_x, seq_y = sequences[i:end_ix, :], sequences[end_ix:out_end_ix, :]\n",
    "        X.append(seq_x)\n",
    "        y.append(seq_y)\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "def rw_validation(df, train_size, valid_size, sliding_distance=1):\n",
    "    # normalization \n",
    "    scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "    df_scaled = scaler.fit_transform(df)\n",
    "    \n",
    "    # convert into input/output\n",
    "    X, y = split_sequences(np.array(df_scaled), train_size, valid_size)\n",
    "\n",
    "    # break scaled data into train and validation datasets\n",
    "    # validation dataset will be comprised of approximately the last 30 days of 2018\n",
    "    n_days_val = 30\n",
    "    n_steps_val = n_days_val*24*4\n",
    "    X_train, y_train = X[:-n_steps_val], y[:-n_steps_val]\n",
    "    X_val, y_val = X[-n_steps_val:], y[-n_steps_val:]\n",
    "\n",
    "    # the dataset knows the number of features\n",
    "    #n_features = X_train.shape[2]\n",
    "    \n",
    "    return X_train, y_train, X_val, y_val, scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prep_smooth_data(df, n_steps_in=[96], n_steps_out=32, window_size=20):\n",
    "    df_list = list()\n",
    "    output_list = list()\n",
    "    for step in n_steps_in:\n",
    "        dfs = data_smooth(df, window=window_size).dropna()\n",
    "        df_list.append(dfs)\n",
    "        x_train, y_train, x_valid, y_valid, scaler = rw_validation(dfs, step, n_steps_out)\n",
    "        output_list.append([x_train, y_train, x_valid, y_valid, scaler, step])\n",
    "    return output_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get a good validation set (only include 12AM-8AM data on each day in last 30 days of 2018)\n",
    "def make_p1_validation(df, n_days_val=30, n_steps_in=n_steps_in, n_steps_out=n_steps_out, all_as_valid=False):\n",
    "    one_day = 24*4\n",
    "    \n",
    "    n_feature = len(df.columns)\n",
    "    \n",
    "    if all_as_valid:\n",
    "        # on all training data\n",
    "        start = one_day\n",
    "    else:\n",
    "        #only validation data\n",
    "        start = len(df)-(n_days_val+1)*one_day\n",
    "        \n",
    "    end = len(df)\n",
    "    \n",
    "    total_len = int((end-start)/one_day)\n",
    "    X_valid = np.empty((total_len, n_steps_in, n_feature))\n",
    "    y_valid = np.empty((total_len, n_steps_out, n_feature))\n",
    "    \n",
    "    count = 0\n",
    "\n",
    "    for i in range(start, end, one_day):\n",
    "        #print(\"i=\", i, \"count= \", count, \" ts: \", df.index[i])\n",
    "        #print(\"X:\", df.iloc[i-n_steps_in:i,:])\n",
    "        #print(\"Y:\", df.iloc[i:i+n_steps_out,:])\n",
    "        X_valid[count] = df.iloc[i-n_steps_in:i,:].values\n",
    "        y_valid[count] = df.iloc[i:i+n_steps_out,:].values\n",
    "        count = count+1\n",
    "    \n",
    "    #print(\"total count: \", count)\n",
    "    return X_valid, y_valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataset_list = prep_smooth_data(df_train, n_steps_in=[96])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_valid, y_valid = make_p1_validation(df_train, all_as_valid=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. LSTM-Seq2Seq Model <a name='m2'></a>\n",
    "[back to Top](#top)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, hidden_size, in_channel=20, sequence_in=64, sequence_out=32, dropout=0, lstm_layer_number=1):\n",
    "        super().__init__()\n",
    "        #n_directions = 1\n",
    "        \n",
    "        self.in_channel = in_channel\n",
    "        self.sequence_in = sequence_in\n",
    "        self.sequence_out = sequence_out\n",
    "        self.hidden_size = hidden_size\n",
    "        self.lstm_layer_number = lstm_layer_number\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        self.lstm = torch.nn.LSTM(in_channel, hidden_size, num_layers=lstm_layer_number, batch_first=True, dropout=dropout)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"Forward compute based on src sequence\"\"\"\n",
    "        batch_size = x.shape[0]\n",
    "        # x: [batch_size, sequence_in, in_channel]\n",
    "        \n",
    "        #apply lstm layer in encoder\n",
    "        init_hidden_cell = (torch.zeros(self.lstm_layer_number,batch_size,self.hidden_size), \n",
    "                       torch.zeros(self.lstm_layer_number,batch_size,self.hidden_size))\n",
    "        outputs, (hidden, cell) = self.lstm(x, init_hidden_cell)\n",
    "        #ouputs: [ batch size,  sequence_in, hidden_size * n_directions]\n",
    "        #hidden, cell: [batch size,  lstm_layer_number* n directions, hidden_size]\n",
    "        \n",
    "        return hidden, cell\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, hidden_size, in_channel=20, sequence_in=64, sequence_out=32, dropout=0, lstm_layer_number=1):\n",
    "        super().__init__()\n",
    "        #n_directions = 1\n",
    "        \n",
    "        self.in_channel = in_channel\n",
    "        self.sequence_in = sequence_in\n",
    "        self.sequence_out = sequence_out\n",
    "        self.hidden_size = hidden_size\n",
    "        self.lstm_layer_number = lstm_layer_number\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        self.lstm = torch.nn.LSTM(in_channel, hidden_size, num_layers=lstm_layer_number, batch_first=True, dropout=dropout)\n",
    "        self.fc_out = nn.Linear(hidden_size, in_channel)\n",
    "    \n",
    "    def forward(self, x, hidden, cell):\n",
    "        # x: [batch_size, in_channel]\n",
    "        x = x.unsqueeze(1)\n",
    "        \n",
    "        # x: [batch_size, 1, in_channel]\n",
    "        output, (hidden, cell) = self.lstm(x, (hidden, cell))\n",
    "        # output: [batch_size, 1, hidden_size]\n",
    "        # hidden, cell:\n",
    "        \n",
    "        prediction = self.fc_out(output.squeeze(1))\n",
    "        # prediction: [batch_size, in_channel]\n",
    "        \n",
    "        return prediction, hidden, cell\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3. Seq2Seq Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, hidden_size, device, in_channel=20, sequence_in=64, sequence_out=32, dropout=0, lstm_layer_number=1):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.in_channel = in_channel\n",
    "        self.sequence_in = sequence_in\n",
    "        self.sequence_out = sequence_out\n",
    "        self.device = device\n",
    "        \n",
    "        self.encoder = Encoder(hidden_size, in_channel=in_channel, sequence_in=sequence_in, sequence_out=sequence_out, dropout=dropout, lstm_layer_number=lstm_layer_number)\n",
    "        self.decoder = Decoder(hidden_size, in_channel=in_channel, sequence_in=sequence_in, sequence_out=sequence_out, dropout=dropout, lstm_layer_number=lstm_layer_number)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # x is source sequence defined in training dataset\n",
    "        # x: [batch_size, sequence_in,  in_channel]\n",
    "        batch_size = x.shape[0]\n",
    "        \n",
    "        # Step-0: intialize the outputs\n",
    "        # outputs: [batch_size, sequence_out, in_channel]\n",
    "        outputs = torch.zeros(self.sequence_out, batch_size, self.in_channel).to(self.device)\n",
    "        \n",
    "        # Step-1: Encoder for input sequence\n",
    "        hidden, cell = self.encoder(x)\n",
    "        \n",
    "        # Step-2: Step-wise apply decoder until sequence_out is reached\n",
    "        # define initial input to decoder\n",
    "        input = x[:,-1,:]\n",
    "        #print(input.shape)\n",
    "        \n",
    "        for i in range(self.sequence_out):\n",
    "            output, hidden, cell = self.decoder(input, hidden, cell)\n",
    "            \n",
    "            outputs[i] = output\n",
    "            \n",
    "            input = output\n",
    "        \n",
    "        return outputs.transpose(0,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4 Train, Valid and Infer Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_fn(model, optimizer, scheduler, loss_fn, dataloader, device):\n",
    "    model.train()\n",
    "    final_loss = 0\n",
    "    \n",
    "    for data in dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        inputs, targets = data['x'].to(device), data['y'].to(device)\n",
    "        outputs = model(inputs)\n",
    "        loss = loss_fn(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        \n",
    "        final_loss += loss.item()\n",
    "        \n",
    "    final_loss /= len(dataloader)\n",
    "    \n",
    "    return final_loss\n",
    "\n",
    "\n",
    "def valid_fn(model, loss_fn, dataloader, device):\n",
    "    model.eval()\n",
    "    final_loss = 0\n",
    "    valid_preds = []\n",
    "    \n",
    "    for data in dataloader:\n",
    "        inputs, targets = data['x'].to(device), data['y'].to(device)\n",
    "        outputs = model(inputs)\n",
    "        loss = loss_fn(outputs, targets)\n",
    "        \n",
    "        final_loss += loss.item()\n",
    "        valid_preds.append(outputs.detach().cpu().numpy())\n",
    "        \n",
    "    final_loss /= len(dataloader)\n",
    "    valid_preds = np.concatenate(valid_preds)\n",
    "    \n",
    "    return final_loss, valid_preds\n",
    "\n",
    "def inference_fn(model, dataloader, device):\n",
    "    model.eval()\n",
    "    preds = []\n",
    "    \n",
    "    for data in dataloader:\n",
    "        inputs = data['x'].to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(inputs)\n",
    "        \n",
    "        preds.append(outputs.detach().cpu().numpy())\n",
    "        \n",
    "    preds = np.concatenate(preds)\n",
    "    \n",
    "    return preds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.5 Dataset Constructor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset:\n",
    "    # targets=None, set up for test dataset\n",
    "    # target is not None, set up for train dataset\n",
    "    def __init__(self, features, targets=None):\n",
    "        self.features = features\n",
    "        self.targets = targets\n",
    "        \n",
    "    def __len__(self):\n",
    "        return (self.features.shape[0])\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        if self.targets is not None:\n",
    "            return {\n",
    "                'x' : torch.tensor(self.features[idx, :], dtype=torch.float),\n",
    "                'y' : torch.tensor(self.targets[idx, :], dtype=torch.float)            \n",
    "            }\n",
    "        else: \n",
    "            return {\n",
    "                'x' : torch.tensor(self.features[idx, :], dtype=torch.float)          \n",
    "            }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.6 LSTM_Seq2Seq Model Object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class lstm_seq_model:\n",
    "    def __init__(self, hidden_size=128, seed=2020, device=('cuda' if torch.cuda.is_available() else 'cpu'), optimizer='swats',  \n",
    "                 epochs=10,  batch_size=128, early_stopping_steps=10, early_stop=True, dropout=0, sequence_in=96, sequence_out=32,\n",
    "                  activation='relu', learning_rate=1e-3, weight_decay=1e-5, lstm_layer_number=1, model_name='CNN.LSTM.BestModel.pth'):\n",
    "        self.hidden_size  = hidden_size\n",
    "        self.seed = seed\n",
    "        self.device = device\n",
    "        self.epochs = epochs\n",
    "        self.batch_size = batch_size\n",
    "        self.early_stopping_steps = early_stopping_steps\n",
    "        self.early_stop = early_stop\n",
    "        self.sequence_in = sequence_in\n",
    "        self.sequence_out = sequence_out\n",
    "        \n",
    "        self.optimizer = optimizer\n",
    "        \n",
    "        self.dropout = dropout\n",
    "        self.activation = activation\n",
    "        self.learning_rate = learning_rate\n",
    "        self.weight_decay = weight_decay\n",
    "        self.lstm_layer_number = lstm_layer_number\n",
    "        self.model_name = model_name\n",
    "        \n",
    "        self.model = Seq2Seq(self.hidden_size, self.device, sequence_in=self.sequence_in, sequence_out=self.sequence_out, lstm_layer_number=self.lstm_layer_number, dropout=dropout).to(device)\n",
    "        self.best_model = Seq2Seq(self.hidden_size, self.device, sequence_in=self.sequence_in, sequence_out=self.sequence_out, lstm_layer_number=self.lstm_layer_number, dropout=dropout).to(device)\n",
    "      \n",
    "    def set_optimizer(self, optimizer):\n",
    "        self.optimizer = optimizer\n",
    "        \n",
    "    def fit(self, X_train, y_train, X_valid, y_valid):\n",
    "        train_dl = torch.utils.data.DataLoader(Dataset(X_train, y_train), batch_size=self.batch_size, shuffle=True, num_workers=1)\n",
    "        valid_dl = torch.utils.data.DataLoader(Dataset(X_valid, y_valid), batch_size=self.batch_size, shuffle=False, num_workers=1)\n",
    "        \n",
    "        if self.optimizer == 'adam':\n",
    "            optimizer = torch.optim.Adam(self.model.parameters(), lr=self.learning_rate, weight_decay=self.weight_decay)\n",
    "        elif self.optimizer == 'swats':\n",
    "            optimizer = SWATS(self.model.parameters(), lr=self.learning_rate, weight_decay=self.weight_decay)\n",
    "        scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer=optimizer, pct_start=0.1, div_factor=1e3, \n",
    "                                                      max_lr=1e-2, epochs=self.epochs, steps_per_epoch=len(train_dl))\n",
    "    \n",
    "        \n",
    "        optimize_loss = nn.L1Loss()  #mae\n",
    "        #optimize_loss = nn.L2Loss() #mse\n",
    "        evaluate_loss = nn.L1Loss()\n",
    "        best_loss = np.inf\n",
    "\n",
    "        #for epoch in tqdm(range(epochs)):\n",
    "        for epoch in range(self.epochs):\n",
    "\n",
    "            train_loss = train_fn(self.model, optimizer, scheduler, optimize_loss, train_dl, self.device)\n",
    "            valid_loss, valid_preds = valid_fn(self.model, evaluate_loss, valid_dl, self.device)\n",
    "            print(f\"EPOCH: {epoch}, train_loss: {train_loss}, valid_loss: {valid_loss}\")\n",
    "\n",
    "            if valid_loss < best_loss:\n",
    "                best_loss = valid_loss\n",
    "                #update best_model\n",
    "                self.best_model.load_state_dict(self.model.state_dict())\n",
    "                #reset early_step to make sure only do early stop for continuous valid_loss > best_loss\n",
    "                early_step = 0 \n",
    "\n",
    "            elif(self.early_stop == True):\n",
    "                early_step += 1\n",
    "                if (early_step >= self.early_stopping_steps):\n",
    "                    break\n",
    "                    \n",
    "    def save_model(self):\n",
    "        torch.save(self.best_model.state_dict(), self.model_name)\n",
    "    \n",
    "    def predict(self, X_test):\n",
    "        test_dl = torch.utils.data.DataLoader(Dataset(X_test), batch_size=1, shuffle=False, num_workers=1)\n",
    "        return inference_fn(self.best_model, test_dl, self.device) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.7 Training LSTM Seq2Seq Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def full_train(dataset_list, hidden_size=128, epochs=60, seeds=[1903, 1], df_train=df_train, optimizer='swats', mode='single'):\n",
    "    \n",
    "    model_list = list()\n",
    "    scaler_list = list()\n",
    "    seq_in_list = list()\n",
    "    X_valid_list = list()\n",
    "    y_valid_list = list()\n",
    "    \n",
    "    for seed in seeds:\n",
    "        seed_everything(seed=seed)\n",
    "        \n",
    "        for dataset in dataset_list:\n",
    "            if mode == 'single':\n",
    "                model = lstm_seq_model(hidden_size=hidden_size, epochs=epochs, optimizer=optimizer, sequence_in=dataset[5], sequence_out=32, model_name='smooth.test.lstm.pth' )\n",
    "                model.fit(dataset[0], dataset[1], dataset[2], dataset[3])\n",
    "            else:\n",
    "                model = lstm_seq_model(hidden_size=hidden_size, epochs=epochs, optimizer='adam', sequence_in=dataset[5], sequence_out=32, model_name='smooth.test.lstm.pth' )\n",
    "                model.fit(dataset[0], dataset[1], dataset[2], dataset[3])\n",
    "                \n",
    "                model.set_optimizer('swats')\n",
    "                model.fit(dataset[0], dataset[1], dataset[2], dataset[3])\n",
    "            \n",
    "            model_list.append(model)\n",
    "            scaler_list.append(dataset[4])\n",
    "            seq_in_list.append(dataset[5])\n",
    "            \n",
    "            X_valid, y_valid = make_p1_validation(df_train, n_steps_in=dataset[5], all_as_valid=True)\n",
    "            X_valid_list.append(X_valid)\n",
    "            y_valid_list.append(y_valid)\n",
    "            \n",
    "        \n",
    "    return model_list, scaler_list, seq_in_list, X_valid_list, y_valid_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model_list, scaler_list, seq_in_list, X_valid_list, y_valid_list = full_train(dataset_list, seeds=[1], epochs=60, optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sw_model_list, sw_scaler_list, sw_seq_in_list, sw_X_valid_list, sw_y_valid_list = full_train(dataset_list, seeds=[1903], epochs=60, optimizer='swats')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.8 Local Evaluation of Model Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_valid(model_list, scaler_list, seq_in_list, X_valid_list, y_valid_list, n_steps_out=32, \n",
    "                   n_features=n_features, raw_valid=True):\n",
    "    \"\"\"Given a list of models, this function evaluate on true valid dataset; only after-scaled data is reported!\"\"\"\n",
    "    eva_loss = nn.L1Loss()\n",
    "    loss_list = list()\n",
    "    pred_list = list()\n",
    "    error_list = list()\n",
    "    \n",
    "    # transform the ground truth to raw data's scale\n",
    "    #y_valid = scaler.inverse_transform(y_valid.reshape(-1,n_features))\n",
    "    \n",
    "    for j, model in enumerate(model_list):\n",
    "        i_scaler = scaler_list[j]\n",
    "        \n",
    "        X_valid = X_valid_list[j]\n",
    "        y_valid = y_valid_list[j]\n",
    "        n_steps_in = seq_in_list[j]\n",
    "        \n",
    "        prediction = np.empty(y_valid.shape)\n",
    "        val_errors = list()\n",
    "        \n",
    "        for i in range(len(X_valid)):\n",
    "            # reshape validation samples and make individual prediction\n",
    "            if raw_valid:\n",
    "                X_val_step = i_scaler.transform(X_valid[i]).reshape((1, n_steps_in, n_features))\n",
    "            else:\n",
    "                X_val_step = X_valid[i].reshape((1, n_steps_in, n_features))\n",
    "                \n",
    "            y_preds = model.predict(X_val_step).reshape(n_steps_out,n_features)\n",
    "            \n",
    "            if raw_valid:\n",
    "                #print(\"at i=\",  i, \" \",  y_valid[i])\n",
    "                #print(i_scaler.inverse_transform(y_preds))\n",
    "                error = mean_absolute_error(i_scaler.inverse_transform(y_preds), y_valid[i])\n",
    "                #print(error)\n",
    "                \n",
    "            val_errors.append(error)\n",
    "            prediction[i] = y_preds\n",
    "        \n",
    "        error_list.append(val_errors)\n",
    "        loss_list.append(np.mean(val_errors))\n",
    "        pred_list.append(prediction)\n",
    "    return loss_list, pred_list, error_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loss, pred, err = evaluate_valid(model_list, scaler_list, seq_in_list, X_valid_list, y_valid_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loss1, pred1, err1 = evaluate_valid(sw_model_list, sw_scaler_list, sw_seq_in_list, sw_X_valid_list, sw_y_valid_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weight_average_valid(model_list, scaler_list, seq_in_list, X_valid_list, y_valid_list, n_steps_out=32, \n",
    "                   n_features=n_features,  weights=None):\n",
    "    if weights is None:\n",
    "        N = len(model_list)\n",
    "        weights = [1/N for i in range(N)]\n",
    "    print(f\"weights: {weights}\")\n",
    "    \n",
    "    \n",
    "    pred_list = list()\n",
    "    loss_list = list()\n",
    "    error_list = list()\n",
    "    \n",
    "    for i, model in enumerate(model_list):\n",
    "        i_scaler = scaler_list[i]\n",
    "        X_valid = X_valid_list[i]\n",
    "        y_valid = y_valid_list[i]\n",
    "        n_steps_in = seq_in_list[i]\n",
    "        \n",
    "        i_prediction = np.empty(y_valid.shape)\n",
    "        val_errors = list()\n",
    "        \n",
    "        for j in range(len(X_valid)):\n",
    "            X_val_step = i_scaler.transform(X_valid[j]).reshape((1, n_steps_in, n_features))\n",
    "            y_preds = model.predict(X_val_step).reshape(n_steps_out,n_features)\n",
    "            y_preds = i_scaler.inverse_transform(y_preds)\n",
    "            error = mean_absolute_error(y_preds, y_valid[j])\n",
    "            \n",
    "            val_errors.append(error)\n",
    "            i_prediction[j] = y_preds\n",
    "        \n",
    "        error_list.append(val_errors)\n",
    "        pred_list.append(i_prediction)\n",
    "        loss_list.append(np.mean(val_errors))\n",
    "    \n",
    "\n",
    "    # Weighted average of prediction\n",
    "    prediction = np.average(pred_list, axis=0, weights=weights)\n",
    "\n",
    "    # compute averaged MAE after weighted averagee\n",
    "    fin_errors=list()\n",
    "    for j in range(len(y_valid_list[0])):\n",
    "        fin_error = mean_absolute_error(prediction[j], y_valid_list[0][j])\n",
    "        fin_errors.append(fin_error)\n",
    "\n",
    "    return prediction, np.mean(fin_errors), fin_errors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Predict for Phase 1 <a name='m3'></a>\n",
    "[back to Top](#top)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Unpickle pre-trained models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#s1_model = load_pkl('models/seed1_adam_96in.pkl')\n",
    "#s1903_model = load_pkl('models/seed1903_swats_96in.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model_list  = [ s1_model['model'], s1903_model['model']]\n",
    "#scaler_list = [ s1_model['scaler'], s1903_model['scaler']]\n",
    "#seq_in_list = [ s1_model['sequence_in'], s1903_model['sequence_in']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Weighted Average for Local Validation\n",
    "#fin_pred, fin_loss, fin_err = weight_average_valid(model_list, scaler_list, seq_in_list, [X_valid,X_valid], [y_valid,y_valid], weights=[0.7,0.3])\n",
    "#fin_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Functions for Phase 1 Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_phase1(model, df_train, scaler, n_steps_in, n_steps_out=n_steps_out, n_features=n_features):\n",
    "    df_test = df_train.iloc[-n_steps_in:, :]\n",
    "    X_test = scaler.transform(df_test).reshape((1, n_steps_in, n_features))\n",
    "    #X_test = np.swapaxes(X_test,1,2)\n",
    "    y_preds = model.predict(X_test).reshape(n_steps_out, n_features)\n",
    "    y_preds = scaler.inverse_transform(y_preds)\n",
    "    df_preds = pd.DataFrame(y_preds, columns=df_train.columns)\n",
    "    \n",
    "    return df_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weight_average_phase1(df_train, model_list, scaler_list, seq_in_list , n_steps_out=32, \n",
    "                   n_features=n_features, weights=None, output_format='pandas'):\n",
    "    if weights is None:\n",
    "        N = len(model_list)\n",
    "        weights = [1/N for i in range(N)]\n",
    "    print(f\"weights: {weights}\")\n",
    "    \n",
    "    pred_list = list()\n",
    "    for i, model in enumerate(model_list):\n",
    "        df_preds = predict_phase1(model, df_train, scaler_list[i], seq_in_list[i], n_steps_out=n_steps_out, \n",
    "                   n_features=n_features)\n",
    "        pred_list.append(df_preds.values)\n",
    "        \n",
    "    prediction = np.average(pred_list, axis=0, weights=weights)\n",
    "    \n",
    "    if output_format == 'numpy':\n",
    "        return prediction\n",
    "    else:\n",
    "        return pd.DataFrame(prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Predict for Phase 2 <a name='m4'></a>\n",
    "[back to Top](#top)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  function to predict by a single model\n",
    "#  reuse the script from starter notebook\n",
    "def predict_phase2(df_phase2, model, scaler, n_steps_in, n_steps_out=32, n_features=20, n_steps_2019 = (31+28+31)*4*24):\n",
    "    # scale dataset\n",
    "    df_phase2_scaled = scaler.transform(df_phase2)\n",
    "    # convert into input/output\n",
    "    # in this case we don't need the outputs\n",
    "    X_phase2, y_phase2 = split_sequences(np.array(df_phase2_scaled),\n",
    "                                         n_steps_in, n_steps_out)\n",
    "    # subset outputs to include only 2019 data\n",
    "    X_2019 = X_phase2[-(n_steps_2019-n_steps_out+1):]\n",
    "    # pre-allocate empty array for predictions\n",
    "    Y_preds = np.empty([X_2019.shape[0], n_steps_out, n_features])\n",
    "\n",
    "    # loop through Q1 2019 data input arrays, make predictions and write back\n",
    "    # to multi-dimensional numpy array\n",
    "    for step, data in enumerate(X_2019):\n",
    "        X_2019_step = data.reshape((1, n_steps_in, n_features))\n",
    "        y_preds = model.predict(X_2019_step).reshape(n_steps_out, n_features)\n",
    "        Y_preds[step] = np.array(scaler.inverse_transform(y_preds))\n",
    "\n",
    "    # output multi-dimensional numpy array of predictions\n",
    "    return Y_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to render predictions for Q1 2019 data\n",
    "\n",
    "# here we are specifying a single input: the Phase II dataframe\n",
    "# teams are welcome to specify multiple function inputs for their custom function so long as they are properly defined and commented\n",
    "\n",
    "def predict_Q1_2019(df_phase2, weights=[0.7,0.3], n_steps_out=32, n_features=20):\n",
    "    \n",
    "    # read pre-trained model from pickle files\n",
    "    s1_model = load_pkl('models/seed1_adam_96in.pkl')\n",
    "    s1903_model = load_pkl('models/seed1903_swats_96in.pkl')\n",
    "    \n",
    "    model_list  = [ s1_model['model'], s1903_model['model']]\n",
    "    scaler_list = [ s1_model['scaler'], s1903_model['scaler']]\n",
    "    seq_in_list = [ s1_model['sequence_in'], s1903_model['sequence_in']]\n",
    "    \n",
    "    # loop over all models and predict\n",
    "    pred_list = list()\n",
    "    #for i, model in enumerate(tqdm(model_list)):\n",
    "    for i, model in enumerate(model_list):\n",
    "        scaler = scaler_list[i]\n",
    "        n_steps_in = seq_in_list[i]\n",
    "        i_preds = predict_phase2(df_phase2, model, scaler, n_steps_in, n_steps_out=n_steps_out, n_features=n_features)\n",
    "        pred_list.append(i_preds)\n",
    "    \n",
    "    # weight average over input models\n",
    "    prediction = np.average(pred_list, axis=0, weights=weights)\n",
    "    \n",
    "    return prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/fyuan2/Virtualenv_Python/amlc2020/lib/python3.7/site-packages/sklearn/base.py:334: UserWarning: Trying to unpickle estimator MinMaxScaler from version 0.22.1 when using version 0.23.2. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  UserWarning)\n"
     ]
    }
   ],
   "source": [
    "Y_preds = predict_Q1_2019(df_phase2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8609, 32, 20)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_preds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if first prediction in Y_preds is consisent with last submission in Phase1\n",
    "def sanity_check(Y_preds, ref_pkl='last.submission.pkl'):\n",
    "    with open(ref_pkl, 'rb') as f:\n",
    "        last_sub = pickle.load(f)\n",
    "    \n",
    "    if np.array_equal(Y_preds[0], last_sub):\n",
    "        return \"Pass Sanity Check\"\n",
    "    else:\n",
    "        return \"Fail Sanity Check\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Pass Sanity Check'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#sanity_check(Y_preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. For Phase II evaluation by AMLC Committee -- NOT USED BY COMPETITORS <a name='m5'></a>\n",
    "[back to Top](#top)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Q1_2019_errors(Y_preds, Y_true):\n",
    "    errors = np.empty(Y_preds.shape[0])\n",
    "    for step, data in enumerate(Y_preds):\n",
    "        errors[step] = mean_absolute_error(data, Y_true[step])\n",
    "    return np.mean(errors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3.7(amlc2020)",
   "language": "python",
   "name": "amlc2020"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
